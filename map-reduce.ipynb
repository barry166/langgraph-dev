{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e608061",
   "metadata": {},
   "source": [
    "# 并行执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73fcd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langgraph langchain_core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88580fcb",
   "metadata": {},
   "source": [
    "## 定义多个流程节点和边"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d90ca81",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m parent_dir = os.path.abspath(os.path.join(current_dir, \u001b[33m'\u001b[39m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m      7\u001b[39m sys.path.append(parent_dir)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menv_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlanggraph_utils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gen_mermaid\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TypedDict\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.abspath(os.path.join(current_dir, '..'))\n",
    "sys.path.append(parent_dir)\n",
    "from utils.env_util import *\n",
    "from langgraph_utils.common_util import gen_mermaid\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing import Annotated\n",
    "from langgraph.types import Send\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9adfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义 Prompt\n",
    "subjects_prompt = \"\"\"后面的对话都使用中文来回答，随机生成4个与 {topic} 相关的关键字，注意只要4个，不要多也不要少，不要重复，生成后你需要自己检查结果是否正确，直接生成结果不需要其他描述。\"\"\"\n",
    "joke_prompt = \"\"\"生成一条关于 {subject} 的笑话，使用中文，只返回一行文字，不要返回多行。最后你需要检查结果是否满足要求，如果有换行符\\\\n你需要去除换行符\"\"\"\n",
    "best_joke_prompt = \"\"\"下面是4行是4个关于 {topic} 的笑话，ID分别是0、1、2、3，选择其中一个并返回其ID，不要多余的分析或描述，最终结果只要返回一个单行的int类型数字，不要包含\\\\n等特殊字符。\n",
    "{jokes}\"\"\"\n",
    "\n",
    "class Subjects(BaseModel):\n",
    "    subjects: list[str]\n",
    "\n",
    "class Joke(BaseModel):\n",
    "    joke: str\n",
    "\n",
    "class BestJoke(BaseModel):\n",
    "    id: int = Field(description=\"序号，int类型，如：1\", ge=0)\n",
    "\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=get_openai_api_key(),\n",
    "    model_name=get_default_model(),\n",
    "    # model_name=\"THUDM/GLM-Z1-32B-0414\",\n",
    "    base_url=get_openai_base_url(),\n",
    "    temperature=0.0,\n",
    ")\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    topic: str\n",
    "    subjects: Annotated[list, operator.add]\n",
    "    jokes: Annotated[list, operator.add]\n",
    "    best_selected_joke: str\n",
    "\n",
    "\n",
    "# 笑话的 subject\n",
    "class JokeState(TypedDict):\n",
    "    subject: str\n",
    "\n",
    "# 通过一个 topic 生成多个 subject\n",
    "def generate_topics(state: OverallState):\n",
    "    prompt = subjects_prompt.format(topic=state[\"topic\"])\n",
    "    print(f'👨 {prompt}')\n",
    "    response = model.with_structured_output(Subjects).invoke(prompt)\n",
    "    print(f\"⚙️生成主题：{response.subjects}\")\n",
    "    return {\"subjects\": response.subjects}\n",
    "\n",
    "def continue_to_jokes(state: OverallState):\n",
    "    \"\"\"\n",
    "        返回一个 `Send` 对象列表\n",
    "        每个 `Send` 对象由图中节点的名称组成\n",
    "        以及发送到该节点的状态\n",
    "\n",
    "        这里是把所有生成的 subject 都发送给 `generate_joke` 生成对应主题的笑话\n",
    "    \"\"\"\n",
    "    return [Send(\"generate_joke\", {\"subject\": s}) for s in state[\"subjects\"]]\n",
    "\n",
    "# 生成一条笑话\n",
    "def generate_joke(joke: JokeState) -> OverallState:\n",
    "    subject = joke[\"subject\"]\n",
    "    prompt = joke_prompt.format(subject=subject)\n",
    "    response = model.with_structured_output(Joke).invoke(prompt)\n",
    "    print(f\"⚙️生成[{subject}]笑话：{response.joke}\")\n",
    "    return {\"jokes\": [response.joke]}\n",
    "\n",
    "def best_joke(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "        从多个笑话中找出1个最好的\n",
    "    \"\"\"\n",
    "    jokes = \"\\n\".join(state[\"jokes\"])\n",
    "    prompt = best_joke_prompt.format(topic=state[\"topic\"], jokes=jokes)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(f'👨 {prompt}')\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # response = model.with_structured_output(BestJoke).invoke(prompt)\n",
    "    response = model.invoke(prompt)\n",
    "    print(f\"⚙️ 选择出了最好的笑话：{response}\")\n",
    "    idx = int(re.findall(r'\\d+', response.content)[0])\n",
    "    print(f\"⚙️ 笑话的ID是 {idx}\")\n",
    "    return {\"best_selected_joke\": state[\"jokes\"][idx]}\n",
    "\n",
    "\n",
    "graph = StateGraph(OverallState)\n",
    "graph.add_node(\"generate_topics\", generate_topics)\n",
    "graph.add_node(\"generate_joke\", generate_joke)\n",
    "graph.add_node(\"best_joke\", best_joke)\n",
    "graph.add_edge(START, \"generate_topics\")\n",
    "graph.add_conditional_edges(\"generate_topics\", continue_to_jokes, [\"generate_joke\"])\n",
    "graph.add_edge(\"generate_joke\", \"best_joke\")\n",
    "graph.add_edge(\"best_joke\", END)\n",
    "app = graph.compile()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.13.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
